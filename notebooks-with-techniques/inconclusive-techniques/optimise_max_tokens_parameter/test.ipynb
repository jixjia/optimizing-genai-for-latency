{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set max_tokens to be close to the expected value\n",
    "\n",
    "The guidance shared in this post was tested using GPT-4. It suggests that setting the max_tokens parameter close to the expected output tokens can help reduce the latency of requests.\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/latency\n",
    "\n",
    "The results were not conclusive, showing only minor differences between the two tests. \n",
    "\n",
    "It is important to consider- the intent of this technique is not to truncate responses that are longer than average. For example, if 90% of responses are less than 200 tokens, and 10% are much longer, setting the max_tokens to 200 tokens will give an improvement to response times. However, this may not give a useful response, as it will be cutoff part way. \n",
    "\n",
    "The expectation is that for responses that output a number of generation tokens below the max_token thershold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Helper Functions and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import copy\n",
    "import textwrap\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# def aoai_call(system_message, prompt, model, max_tokens):\n",
    "#     client = AzureOpenAI(\n",
    "#         api_version=os.getenv(\"API_VERSION\"),\n",
    "#         azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "#         api_key=os.getenv(\"API_KEY\")\n",
    "#     )\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     completion = client.chat.completions.create(\n",
    "#         model=model,\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": system_message},\n",
    "#             {\"role\": \"user\", \"content\": prompt},\n",
    "#         ],\n",
    "#         max_tokens=max_tokens\n",
    "#     )\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     e2e_time = end_time - start_time\n",
    "\n",
    "#     result = json.loads(completion.model_dump_json(indent=2))\n",
    "#     prompt_tokens = result[\"usage\"][\"prompt_tokens\"]\n",
    "#     completion_tokens = result[\"usage\"][\"completion_tokens\"]\n",
    "#     completion_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "#     return result, prompt_tokens, completion_tokens, completion_text, e2e_time\n",
    "model=os.getenv(\"MODELGPT4-8k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case: RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_documents=\"\"\"\n",
    "\n",
    "1. “Quantum Entanglement: Spooky Action at a Distance”\n",
    "Abstract:\n",
    "Quantum entanglement, a phenomenon that baffled even Einstein, lies at the heart of quantum mechanics. In this article, we delve into the mysterious world of entangled particles, exploring how they can be connected across vast distances instantaneously. From Bell’s theorem to quantum teleportation, we unravel the enigma of entanglement and its potential applications in quantum computing and secure communication.\n",
    "\n",
    "Introduction:\n",
    "Quantum entanglement defies classical intuition. Imagine two particles—say, electrons—created together and then separated by light-years. Remarkably, their properties remain intertwined, regardless of the distance between them. When one particle’s state changes, the other responds instantaneously, as if they share a hidden connection. But how does this “spooky action at a distance” work?\n",
    "\n",
    "Bell’s Theorem:\n",
    "Physicist John Bell proposed a test to determine whether entanglement was real or merely a statistical fluke. Experiments confirmed Bell’s predictions: the correlations between entangled particles violated classical limits. Quantum mechanics prevailed, and entanglement emerged as a fundamental property of the universe.\n",
    "\n",
    "Quantum Teleportation:\n",
    "Entanglement enables quantum teleportation—a process where information about one particle is transmitted to another, even if they are light-years apart. This isn’t “Star Trek” teleportation of matter; instead, it transfers quantum states. Researchers are harnessing this phenomenon for secure communication and quantum networks.\n",
    "\n",
    "Applications:\n",
    "Beyond teleportation, entanglement plays a pivotal role in quantum computing. Qubits, the building blocks of quantum computers, rely on entanglement for their power. Scientists are also exploring entanglement-based sensors, clocks, and cryptography.\n",
    "\n",
    "2. “CRISPR-Cas9: Rewriting the Genetic Code”\n",
    "Abstract:\n",
    "CRISPR-Cas9, a revolutionary gene-editing tool, has transformed biology and medicine. In this article, we explore the origins of CRISPR, its mechanism, and its impact on genetic research. From curing genetic diseases to creating designer organisms, CRISPR opens new frontiers in biotechnology.\n",
    "\n",
    "Introduction:\n",
    "Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) were initially discovered in bacteria as part of their immune system. Scientists soon realized that they could repurpose this system for precise gene editing. Enter CRISPR-Cas9—the Swiss Army knife of genetic manipulation.\n",
    "\n",
    "How It Works:\n",
    "CRISPR-Cas9 acts like molecular scissors. It uses a guide RNA to target specific DNA sequences, and the Cas9 protein cuts the DNA at that location. Researchers can then insert, delete, or modify genes with unprecedented accuracy. The simplicity and efficiency of CRISPR have revolutionized genetic research.\n",
    "\n",
    "Applications:\n",
    "Treating Genetic Diseases: CRISPR holds promise for curing genetic disorders like sickle cell anemia and cystic fibrosis. Clinical trials are underway.\n",
    "Agriculture: CRISPR can create crops resistant to pests, drought, and disease.\n",
    "Designer Babies?: Ethical debates surround using CRISPR for human enhancement.\n",
    "Conservation: CRISPR may help save endangered species by editing their genomes.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A: Setting max_tokens to 2000\n",
    "\n",
    "**Time taken: 1.9 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.94 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.66 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.82 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.58 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.99 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.90 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 2.58 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.78 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.70 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 2.09 seconds\n",
      "A\n",
      "Average time taken: 1.90 seconds\n"
     ]
    }
   ],
   "source": [
    "# user_question=\" What is Cripsr?\"\n",
    "\n",
    "# system_message=\"\"\"\n",
    "# You are a helpful AI assistant.\n",
    "# \"\"\"\n",
    "# prompt=f\"\"\"\n",
    "# context: {context_documents}\n",
    "# Return the character \"A\".\n",
    "# \"\"\"\n",
    "\n",
    "# e2e_times = []\n",
    "# for _ in range(10):\n",
    "#     result,prompt_tokens,completion_tokens,completion_text,e2e_time=aoai_call(system_message,prompt,model,2000)\n",
    "#     print(f\"Prompt Tokens: {prompt_tokens}\")\n",
    "#     print(f\"Completion Tokens: {completion_tokens}\")\n",
    "#     print(f\"Time taken: {e2e_time:.2f} seconds\")\n",
    "#     print(completion_text)\n",
    "#     e2e_times.append(e2e_time)\n",
    "\n",
    "# average_e2e_time = sum(e2e_times) / len(e2e_times)\n",
    "# print(f\"Average time taken: {average_e2e_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B: Set max_tokens to 50 tokens\n",
    "\n",
    "**Time taken: 1.7 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.85 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.79 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 2.43 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.74 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.48 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.70 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.57 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.96 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.52 seconds\n",
      "A\n",
      "Prompt Tokens: 675\n",
      "Completion Tokens: 1\n",
      "Total cost: $0.0406\n",
      "Time taken: 1.52 seconds\n",
      "A\n",
      "Average time taken: 1.76 seconds\n"
     ]
    }
   ],
   "source": [
    "# user_question=\" What is Cripsr?\"\n",
    "\n",
    "# system_message=\"\"\"\n",
    "# You are a helpful AI assistant.\n",
    "# \"\"\"\n",
    "# prompt=f\"\"\"\n",
    "# context: {context_documents}\n",
    "# Return the character \"A\".\n",
    "# \"\"\"\n",
    "\n",
    "# e2e_times = []\n",
    "# for _ in range(10):\n",
    "#     result,prompt_tokens,completion_tokens,completion_text,e2e_time=aoai_call(system_message,prompt,model,50)\n",
    "#     print(f\"Prompt Tokens: {prompt_tokens}\")\n",
    "#     print(f\"Completion Tokens: {completion_tokens}\")\n",
    "#     print(f\"Time taken: {e2e_time:.2f} seconds\")\n",
    "#     print(completion_text)\n",
    "#     e2e_times.append(e2e_time)\n",
    "\n",
    "# average_e2e_time = sum(e2e_times) / len(e2e_times)\n",
    "# print(f\"Average time taken: {average_e2e_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A: TBC\n",
    "\n",
    "**Time taken: 1.9 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Stream' object has no attribute 'model_dump_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you generate a creative poem about the moon?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 38\u001b[0m result, _, _, generated_poem, _ \u001b[38;5;241m=\u001b[39m \u001b[43maoai_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_poem)\n",
      "Cell \u001b[0;32mIn[23], line 27\u001b[0m, in \u001b[0;36maoai_call\u001b[0;34m(system_message, prompt, model, max_tokens)\u001b[0m\n\u001b[1;32m     24\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     25\u001b[0m e2e_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m---> 27\u001b[0m result \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[43mcompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump_json\u001b[49m(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     28\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     29\u001b[0m completion_tokens \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Stream' object has no attribute 'model_dump_json'"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import json\n",
    "\n",
    "# def aoai_call(system_message, prompt, model, max_tokens):\n",
    "#     client = AzureOpenAI(\n",
    "#         api_version=os.getenv(\"API_VERSION\"),\n",
    "#         azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "#         api_key=os.getenv(\"API_KEY\")\n",
    "#     )\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     completion = client.chat.completions.create(\n",
    "#         model=model,\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": system_message},\n",
    "#             {\"role\": \"user\", \"content\": prompt},\n",
    "#         ],\n",
    "#         stream=True,\n",
    "#         max_tokens=max_tokens\n",
    "#     )\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     e2e_time = end_time - start_time\n",
    "\n",
    "#     result = json.loads(completion.model_dump_json(indent=2))\n",
    "#     prompt_tokens = result[\"usage\"][\"prompt_tokens\"]\n",
    "#     completion_tokens = result[\"usage\"][\"completion_tokens\"]\n",
    "#     completion_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "#     return result, prompt_tokens, completion_tokens, completion_text, e2e_time\n",
    "\n",
    "# # Example usage:\n",
    "# system_message = \"Welcome to the Azure OpenAI chat!\"\n",
    "# prompt = \"Can you generate a creative poem about the moon?\"\n",
    "# max_tokens = 50\n",
    "# result, _, _, generated_poem, _ = aoai_call(system_message, prompt, model, max_tokens)\n",
    "# print(generated_poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 0: None\n",
      "Message 1: CR\n",
      "Message 2: IS\n",
      "Message 3: PR\n",
      "Message 4: ,\n",
      "Message 5:  also\n",
      "Message 6:  known\n",
      "Message 7:  as\n",
      "Message 8:  CR\n",
      "Message 9: IS\n",
      "Message 10: PR\n",
      "Message 11: -C\n",
      "Message 12: as\n",
      "Message 13: 9\n",
      "Message 14: ,\n",
      "Message 15:  is\n",
      "Message 16:  a\n",
      "Message 17:  revolutionary\n",
      "Message 18:  gene\n",
      "Message 19: -edit\n",
      "Message 20: ing\n",
      "Message 21:  tool\n",
      "Message 22:  that\n",
      "Message 23:  has\n",
      "Message 24:  significantly\n",
      "Message 25:  transformed\n",
      "Message 26:  fields\n",
      "Message 27:  like\n",
      "Message 28:  biology\n",
      "Message 29:  and\n",
      "Message 30:  medicine\n",
      "Message 31: .\n",
      "Message 32:  The\n",
      "Message 33:  term\n",
      "Message 34:  \"\n",
      "Message 35: CR\n",
      "Message 36: IS\n",
      "Message 37: PR\n",
      "Message 38: \"\n",
      "Message 39:  stands\n",
      "Message 40:  for\n",
      "Message 41:  Cluster\n",
      "Message 42: ed\n",
      "Message 43:  Regular\n",
      "Message 44: ly\n",
      "Message 45:  Int\n",
      "Message 46: ers\n",
      "Message 47: paced\n",
      "Message 48:  Short\n",
      "Message 49:  Pal\n",
      "Message 50: ind\n",
      "Message 51: rom\n",
      "Message 52: ic\n",
      "Message 53:  Re\n",
      "Message 54: peats\n",
      "Message 55: ,\n",
      "Message 56:  which\n",
      "Message 57:  were\n",
      "Message 58:  originally\n",
      "Message 59:  identified\n",
      "Message 60:  in\n",
      "Message 61:  bacteria\n",
      "Message 62:  as\n",
      "Message 63:  a\n",
      "Message 64:  part\n",
      "Message 65:  of\n",
      "Message 66:  their\n",
      "Message 67:  immune\n",
      "Message 68:  system\n",
      "Message 69: .\n",
      "Message 70:  Scientists\n",
      "Message 71:  soon\n",
      "Message 72:  discovered\n",
      "Message 73:  that\n",
      "Message 74:  they\n",
      "Message 75:  could\n",
      "Message 76:  harness\n",
      "Message 77:  this\n",
      "Message 78:  system\n",
      "Message 79:  for\n",
      "Message 80:  accurate\n",
      "Message 81:  and\n",
      "Message 82:  efficient\n",
      "Message 83:  gene\n",
      "Message 84: -edit\n",
      "Message 85: ing\n",
      "Message 86:  purposes\n",
      "Message 87: .\n",
      "Message 88:  \n",
      "\n",
      "\n",
      "Message 89: CR\n",
      "Message 90: IS\n",
      "Message 91: PR\n",
      "Message 92: -C\n",
      "Message 93: as\n",
      "Message 94: 9\n",
      "Message 95:  functions\n",
      "Message 96:  like\n",
      "Message 97:  molecular\n",
      "Message 98:  scissors\n",
      "Message 99: ,\n",
      "Message 100:  using\n",
      "Message 101:  a\n",
      "Message 102:  guide\n",
      "Message 103:  RNA\n",
      "Message 104:  to\n",
      "Message 105:  target\n",
      "Message 106:  specific\n",
      "Message 107:  DNA\n",
      "Message 108:  sequences\n",
      "Message 109: ,\n",
      "Message 110:  after\n",
      "Message 111:  which\n",
      "Message 112:  the\n",
      "Message 113:  Cas\n",
      "Message 114: 9\n",
      "Message 115:  protein\n",
      "Message 116:  cuts\n",
      "Message 117:  the\n",
      "Message 118:  DNA\n",
      "Message 119:  at\n",
      "Message 120:  the\n",
      "Message 121:  identified\n",
      "Message 122:  location\n",
      "Message 123: .\n",
      "Message 124:  This\n",
      "Message 125:  allows\n",
      "Message 126:  researchers\n",
      "Message 127:  to\n",
      "Message 128:  insert\n",
      "Message 129: ,\n",
      "Message 130:  delete\n",
      "Message 131: ,\n",
      "Message 132:  or\n",
      "Message 133:  modify\n",
      "Message 134:  genes\n",
      "Message 135:  with\n",
      "Message 136:  exceptional\n",
      "Message 137:  accuracy\n",
      "Message 138: .\n",
      "Message 139:  The\n",
      "Message 140:  applications\n",
      "Message 141:  of\n",
      "Message 142:  CR\n",
      "Message 143: IS\n",
      "Message 144: PR\n",
      "Message 145:  are\n",
      "Message 146:  wide\n",
      "Message 147:  and\n",
      "Message 148:  varied\n",
      "Message 149:  -\n",
      "Message 150:  it\n",
      "Message 151:  can\n",
      "Message 152:  potentially\n",
      "Message 153:  cure\n",
      "Message 154:  genetic\n",
      "Message 155:  disorders\n",
      "Message 156:  like\n",
      "Message 157:  sick\n",
      "Message 158: le\n",
      "Message 159:  cell\n",
      "Message 160:  an\n",
      "Message 161: emia\n",
      "Message 162:  and\n",
      "Message 163:  cyst\n",
      "Message 164: ic\n",
      "Message 165:  fib\n",
      "Message 166: rosis\n",
      "Message 167: ,\n",
      "Message 168:  aid\n",
      "Message 169:  in\n",
      "Message 170:  creating\n",
      "Message 171:  crops\n",
      "Message 172:  resistant\n",
      "Message 173:  to\n",
      "Message 174:  pests\n",
      "Message 175: ,\n",
      "Message 176:  drought\n",
      "Message 177: ,\n",
      "Message 178:  and\n",
      "Message 179:  disease\n",
      "Message 180: ,\n",
      "Message 181:  and\n",
      "Message 182:  may\n",
      "Message 183:  even\n",
      "Message 184:  help\n",
      "Message 185:  in\n",
      "Message 186:  conservation\n",
      "Message 187:  efforts\n",
      "Message 188:  by\n",
      "Message 189:  editing\n",
      "Message 190:  the\n",
      "Message 191:  genomes\n",
      "Message 192:  of\n",
      "Message 193:  endangered\n",
      "Message 194:  species\n",
      "Message 195: .\n",
      "Message 196:  However\n",
      "Message 197: ,\n",
      "Message 198:  ethical\n",
      "Message 199:  debates\n",
      "Message 200:  are\n",
      "Message 201:  ongoing\n",
      "Message 202:  around\n",
      "Message 203:  the\n",
      "Message 204:  use\n",
      "Message 205:  of\n",
      "Message 206:  CR\n",
      "Message 207: IS\n",
      "Message 208: PR\n",
      "Message 209:  for\n",
      "Message 210:  human\n",
      "Message 211:  enhancement\n",
      "Message 212: ,\n",
      "Message 213:  sometimes\n",
      "Message 214:  referred\n",
      "Message 215:  to\n",
      "Message 216:  as\n",
      "Message 217:  creating\n",
      "Message 218:  \"\n",
      "Message 219: design\n",
      "Message 220: er\n",
      "Message 221:  babies\n",
      "Message 222: \".\n",
      "\n",
      "Message 223: None\n",
      "Average time between tokens: 0.046110647065298896 seconds\n"
     ]
    }
   ],
   "source": [
    "# estimated_max_tokens=2000\n",
    "\n",
    "# user_question=\" What is crispr?\"\n",
    "\n",
    "# system_message=\"\"\"\n",
    "# You help write a summary of this document. Explain responses in detail.\n",
    "# \"\"\"\n",
    "# prompt=f\"\"\"\n",
    "# Context for answering the question:\n",
    "# {context_documents}\n",
    "# User question:\n",
    "# {user_question}\n",
    "# \"\"\"\n",
    "\n",
    "# client = AzureOpenAI(\n",
    "#     api_version=os.getenv(\"API_VERSION\"),\n",
    "#     azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "#     api_key=os.getenv(\"API_KEY\")\n",
    "# )\n",
    "\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#     model=model,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": system_message},\n",
    "#         {\"role\": \"user\", \"content\": prompt},\n",
    "#     ],\n",
    "#     stream=True,\n",
    "#     max_tokens=estimated_max_tokens\n",
    "# )\n",
    "\n",
    "# e2e_start_time = time.time()\n",
    "# import time\n",
    "\n",
    "# tbt_durations = []\n",
    "# previous_time = time.time()\n",
    "\n",
    "\n",
    "# for i, message in enumerate(completion):\n",
    "#     if i==1:\n",
    "#         time_to_first_token=time.time()-e2e_start_time\n",
    "\n",
    "#     current_time = time.time()\n",
    "#     tbt_durations.append(current_time - previous_time)\n",
    "#     previous_time = current_time\n",
    "#     # print(f\"Message {i}: {message.choices[0].delta.content}\")\n",
    "\n",
    "# average_tbt_duration = sum(tbt_durations) / len(tbt_durations)\n",
    "# # print(f\"Average time between tokens: {average_tbt_duration} seconds\")\n",
    "\n",
    "# e2e_end_time = time.time()\n",
    "# e2e_time = e2e_end_time - e2e_start_time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End to end time:  10.329296827316284\n",
      "Time to first token:  1.4505589008331299\n",
      "Average time between tokens:  0.046110647065298896\n"
     ]
    }
   ],
   "source": [
    "# print(\"End to end time: \", e2e_time)\n",
    "# print(\"Time to first token: \", time_to_first_token)\n",
    "# print(\"Average time between tokens: \", average_tbt_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message received 1.34 seconds after request: None\n",
      "Message received 1.37 seconds after request: A\n",
      "Message received 1.37 seconds after request: None\n",
      "Full response received 1.37 seconds after request\n",
      "Full conversation received: A\n"
     ]
    }
   ],
   "source": [
    "# # record the time before the request is sent\n",
    "# start_time = time.time()\n",
    "\n",
    "# # send a ChatCompletion request to count to 100\n",
    "# response = client.chat.completions.create(\n",
    "#     model=model,\n",
    "#     messages=[\n",
    "#         {'role': 'user', 'content': 'return the character \"A\" with no other characters.'}\n",
    "#     ],\n",
    "#     temperature=0,\n",
    "#     stream=True  # again, we set stream=True\n",
    "# )\n",
    "# # create variables to collect the stream of chunks\n",
    "# collected_chunks = []\n",
    "# collected_messages = []\n",
    "# # iterate through the stream of events\n",
    "# for chunk in response:\n",
    "#     chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n",
    "#     collected_chunks.append(chunk)  # save the event response\n",
    "#     chunk_message = chunk.choices[0].delta.content  # extract the message\n",
    "#     collected_messages.append(chunk_message)  # save the message\n",
    "#     print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text\n",
    "\n",
    "# # print the time delay and text received\n",
    "# print(f\"Full response received {chunk_time:.2f} seconds after request\")\n",
    "# # clean None in collected_messages\n",
    "# collected_messages = [m for m in collected_messages if m is not None]\n",
    "# full_reply_content = ''.join([m for m in collected_messages])\n",
    "# print(f\"Full conversation received: {full_reply_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "client = AzureOpenAI(\n",
    "    api_version=os.getenv(\"API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"API_KEY\")\n",
    ")\n",
    "\n",
    "# Initialize DataFrame\n",
    "\n",
    "def run_experiment(max_tokens,samples):\n",
    "    df = pd.DataFrame(columns=['e2e_time', 'time_to_first_token', 'average_tbt_duration','achieved_completion_chunks'])\n",
    "    \n",
    "\n",
    "    user_question=\"What is crispr?\"\n",
    "\n",
    "    system_message=\"\"\"\n",
    "    You help write a summary of this document. Explain responses in detail.\n",
    "    \"\"\"\n",
    "    prompt=f\"\"\"\n",
    "    Context for answering the question:\n",
    "    {context_documents}\n",
    "    User question:\n",
    "    {user_question}\n",
    "    \"\"\"\n",
    "    for _ in range(samples):\n",
    "        start_time = time.time()\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            stream=True,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "\n",
    "        e2e_start_time = time.time()\n",
    "\n",
    "        tbt_durations = []\n",
    "        previous_time = time.time()\n",
    "\n",
    "        for i, message in enumerate(completion):\n",
    "            if i==1:\n",
    "                time_to_first_token=time.time()-e2e_start_time\n",
    "\n",
    "            current_time = time.time()\n",
    "            tbt_durations.append(current_time - previous_time)\n",
    "            previous_time = current_time\n",
    "\n",
    "        average_tbt_duration = sum(tbt_durations) / len(tbt_durations)\n",
    "\n",
    "        e2e_end_time = time.time()\n",
    "        e2e_time = e2e_end_time - e2e_start_time\n",
    "        # print(time_to_first_token)\n",
    "        # print(tbt_durations)\n",
    "\n",
    "        # Create a DataFrame for the current row\n",
    "        current_df = pd.DataFrame({\n",
    "            'e2e_time': [e2e_time],\n",
    "            'time_to_first_token': [time_to_first_token],\n",
    "            'average_tbt_duration': [average_tbt_duration],\n",
    "            'achieved_completion_chunks': len(tbt_durations),\n",
    "        })\n",
    "\n",
    "        # Concatenate the current DataFrame with the main DataFrame\n",
    "        df = pd.concat([df, current_df], ignore_index=True)\n",
    "\n",
    "    # Print DataFrame\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/04/byk1dtp95sn0gjlx1gsgdqkw0000gn/T/ipykernel_13895/2773086162.py:62: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, current_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     e2e_time  time_to_first_token  average_tbt_duration  \\\n",
      "0    6.809158             1.252731              0.041265   \n",
      "1   10.628409             1.538930              0.058025   \n",
      "2    6.501748             1.245632              0.034876   \n",
      "3   10.789815             3.012136              0.065392   \n",
      "4    5.754230             1.254783              0.035513   \n",
      "5    6.065122             1.311185              0.037438   \n",
      "6    6.913838             1.383482              0.041525   \n",
      "7    6.175123             1.242275              0.038594   \n",
      "8    5.657351             1.075053              0.030409   \n",
      "9    8.330798             2.386161              0.054276   \n",
      "10  20.318726             3.176152              0.083065   \n",
      "11  10.647999             1.950534              0.070882   \n",
      "12  13.326495             2.315608              0.076188   \n",
      "13   7.042649             1.763406              0.052270   \n",
      "14   6.672189             1.443635              0.040191   \n",
      "15   4.724957             1.149270              0.034995   \n",
      "16   7.511572             2.116330              0.050776   \n",
      "17   6.882346             1.466203              0.044197   \n",
      "18  11.991621             2.872477              0.082130   \n",
      "19  19.148333             3.453015              0.089757   \n",
      "20   7.796491             0.889888              0.037482   \n",
      "21   8.065475             2.374074              0.050407   \n",
      "22   8.338975             1.712993              0.049312   \n",
      "23   6.499954             1.326227              0.041905   \n",
      "24   7.390865             1.857986              0.049600   \n",
      "25   9.397331             2.348299              0.062758   \n",
      "26   5.313231             1.023585              0.032590   \n",
      "27   6.706645             1.046362              0.033700   \n",
      "28   8.266451             1.730591              0.044925   \n",
      "29   8.223132             1.787739              0.045938   \n",
      "30  12.747951             3.146924              0.087062   \n",
      "31   4.742771             1.549454              0.030858   \n",
      "32  11.937320             1.235561              0.056779   \n",
      "33  11.147281             3.031639              0.073783   \n",
      "34   7.451075             1.486591              0.043602   \n",
      "35  12.827053             2.207810              0.074086   \n",
      "36   6.904401             1.518269              0.043149   \n",
      "37   8.548405             1.373627              0.045712   \n",
      "38   4.082051             0.911301              0.029366   \n",
      "39   9.978075             1.333424              0.048203   \n",
      "40   7.320486             1.190522              0.048133   \n",
      "41   7.968700             1.373389              0.048588   \n",
      "42   6.014698             1.310612              0.033207   \n",
      "43   5.732247             1.400893              0.037201   \n",
      "44   5.415665             1.529050              0.036783   \n",
      "45   5.882898             1.928967              0.044564   \n",
      "46   8.247456             1.295133              0.046762   \n",
      "47   7.498275             1.642538              0.047158   \n",
      "48   9.048104             1.661552              0.045899   \n",
      "49   7.346945             1.311628              0.041703   \n",
      "\n",
      "   achieved_completion_chunks  \n",
      "0                         165  \n",
      "1                         179  \n",
      "2                         185  \n",
      "3                         165  \n",
      "4                         159  \n",
      "5                         162  \n",
      "6                         159  \n",
      "7                         160  \n",
      "8                         176  \n",
      "9                         148  \n",
      "10                        241  \n",
      "11                        147  \n",
      "12                        171  \n",
      "13                        129  \n",
      "14                        166  \n",
      "15                        135  \n",
      "16                        142  \n",
      "17                        149  \n",
      "18                        146  \n",
      "19                        210  \n",
      "20                        208  \n",
      "21                        160  \n",
      "22                        163  \n",
      "23                        148  \n",
      "24                        143  \n",
      "25                        145  \n",
      "26                        154  \n",
      "27                        199  \n",
      "28                        184  \n",
      "29                        179  \n",
      "30                        143  \n",
      "31                        144  \n",
      "32                        205  \n",
      "33                        147  \n",
      "34                        164  \n",
      "35                        169  \n",
      "36                        160  \n",
      "37                        187  \n",
      "38                        139  \n",
      "39                        207  \n",
      "40                        146  \n",
      "41                        164  \n",
      "42                        172  \n",
      "43                        146  \n",
      "44                        138  \n",
      "45                        132  \n",
      "46                        170  \n",
      "47                        159  \n",
      "48                        191  \n",
      "49                        169  \n"
     ]
    }
   ],
   "source": [
    "high_max_tokens_df=run_experiment(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/04/byk1dtp95sn0gjlx1gsgdqkw0000gn/T/ipykernel_13895/2773086162.py:62: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, current_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     e2e_time  time_to_first_token  average_tbt_duration  \\\n",
      "0    8.998691             1.853194              0.062596   \n",
      "1    8.529188             2.194671              0.048973   \n",
      "2   12.616259             1.666113              0.067106   \n",
      "3    7.767991             1.487084              0.042506   \n",
      "4    7.624175             1.655109              0.048253   \n",
      "5   12.936478             4.686015              0.071079   \n",
      "6    7.431397             1.552279              0.046618   \n",
      "7    7.317405             1.354394              0.041769   \n",
      "8    8.854402             1.554690              0.047597   \n",
      "9    5.599775             1.284790              0.036811   \n",
      "10   9.313534             1.810169              0.050489   \n",
      "11   7.723169             2.021707              0.052649   \n",
      "12   8.578085             1.351509              0.045318   \n",
      "13   7.665675             1.536036              0.043545   \n",
      "14   9.132491             2.989842              0.054684   \n",
      "15  12.035122             2.228785              0.067838   \n",
      "16   6.592739             1.193375              0.043307   \n",
      "17  10.145969             1.652007              0.053962   \n",
      "18   8.172697             1.388993              0.048887   \n",
      "19   7.327177             1.709819              0.057692   \n",
      "20   9.149629             2.596544              0.059748   \n",
      "21   8.782540             1.518428              0.049015   \n",
      "22   7.053536             1.730281              0.040486   \n",
      "23   7.261886             1.512724              0.048331   \n",
      "24   7.913118             1.771194              0.052306   \n",
      "25   9.794579             1.981671              0.057954   \n",
      "26   9.402212             1.454093              0.050303   \n",
      "27   6.523048             1.505647              0.042623   \n",
      "28   5.963751             1.206689              0.037231   \n",
      "29   7.702883             1.747682              0.049698   \n",
      "30   5.918770             1.613360              0.038016   \n",
      "31   7.370745             1.715259              0.046945   \n",
      "32   7.318258             1.285758              0.046247   \n",
      "33   7.485325             1.590770              0.049154   \n",
      "34   6.358580             1.293186              0.039854   \n",
      "35   6.354261             1.641075              0.040623   \n",
      "36   3.574954             0.933505              0.026676   \n",
      "37   7.568390             1.792596              0.056904   \n",
      "38   8.255643             0.937365              0.043680   \n",
      "39   4.788931             0.877036              0.026881   \n",
      "40   6.173589             1.537804              0.045728   \n",
      "41   8.815858             1.875035              0.049810   \n",
      "42  12.093205             1.818832              0.075582   \n",
      "43   9.077985             2.147345              0.051138   \n",
      "44   8.442337             1.831436              0.048763   \n",
      "45   6.704606             1.328048              0.043566   \n",
      "46  10.834175             1.310444              0.050664   \n",
      "47   7.852343             1.507736              0.044173   \n",
      "48   8.080007             1.797075              0.053614   \n",
      "49   6.800632             1.606542              0.043315   \n",
      "\n",
      "   achieved_completion_chunks  \n",
      "0                         139  \n",
      "1                         168  \n",
      "2                         188  \n",
      "3                         175  \n",
      "4                         158  \n",
      "5                         182  \n",
      "6                         153  \n",
      "7                         168  \n",
      "8                         185  \n",
      "9                         144  \n",
      "10                        180  \n",
      "11                        141  \n",
      "12                        187  \n",
      "13                        172  \n",
      "14                        167  \n",
      "15                        173  \n",
      "16                        147  \n",
      "17                        187  \n",
      "18                        161  \n",
      "19                        127  \n",
      "20                        153  \n",
      "21                        173  \n",
      "22                        166  \n",
      "23                        144  \n",
      "24                        145  \n",
      "25                        169  \n",
      "26                        181  \n",
      "27                        146  \n",
      "28                        154  \n",
      "29                        149  \n",
      "30                        153  \n",
      "31                        157  \n",
      "32                        158  \n",
      "33                        147  \n",
      "34                        152  \n",
      "35                        150  \n",
      "36                        134  \n",
      "37                        133  \n",
      "38                        189  \n",
      "39                        167  \n",
      "40                        135  \n",
      "41                        171  \n",
      "42                        160  \n",
      "43                        174  \n",
      "44                        167  \n",
      "45                        150  \n",
      "46                        207  \n",
      "47                        171  \n",
      "48                        148  \n",
      "49                        157  \n"
     ]
    }
   ],
   "source": [
    "low_max_tokens_df=run_experiment(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e2e_time</th>\n",
       "      <th>time_to_first_token</th>\n",
       "      <th>average_tbt_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.374658</td>\n",
       "      <td>1.718913</td>\n",
       "      <td>0.049460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.231526</td>\n",
       "      <td>0.642517</td>\n",
       "      <td>0.015634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.082051</td>\n",
       "      <td>0.889888</td>\n",
       "      <td>0.029366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.474675</td>\n",
       "      <td>1.502430</td>\n",
       "      <td>0.045805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>12.067254</td>\n",
       "      <td>2.886443</td>\n",
       "      <td>0.074296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>13.101746</td>\n",
       "      <td>3.095046</td>\n",
       "      <td>0.082644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>19.745233</td>\n",
       "      <td>3.317352</td>\n",
       "      <td>0.088437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.318726</td>\n",
       "      <td>3.453015</td>\n",
       "      <td>0.089757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        e2e_time  time_to_first_token  average_tbt_duration\n",
       "count  50.000000            50.000000             50.000000\n",
       "mean    8.374658             1.718913              0.049460\n",
       "std     3.231526             0.642517              0.015634\n",
       "min     4.082051             0.889888              0.029366\n",
       "50%     7.474675             1.502430              0.045805\n",
       "90%    12.067254             2.886443              0.074296\n",
       "95%    13.101746             3.095046              0.082644\n",
       "99%    19.745233             3.317352              0.088437\n",
       "max    20.318726             3.453015              0.089757"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_max_tokens_df.describe(percentiles = [.5, 0.9, .95, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e2e_time</th>\n",
       "      <th>time_to_first_token</th>\n",
       "      <th>average_tbt_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.075564</td>\n",
       "      <td>1.692715</td>\n",
       "      <td>0.048814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.864304</td>\n",
       "      <td>0.576373</td>\n",
       "      <td>0.009568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.574954</td>\n",
       "      <td>0.877036</td>\n",
       "      <td>0.026676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.745580</td>\n",
       "      <td>1.609951</td>\n",
       "      <td>0.048547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>10.214790</td>\n",
       "      <td>2.152078</td>\n",
       "      <td>0.060033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>12.067068</td>\n",
       "      <td>2.431052</td>\n",
       "      <td>0.067509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>12.779571</td>\n",
       "      <td>3.854890</td>\n",
       "      <td>0.073375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.936478</td>\n",
       "      <td>4.686015</td>\n",
       "      <td>0.075582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        e2e_time  time_to_first_token  average_tbt_duration\n",
       "count  50.000000            50.000000             50.000000\n",
       "mean    8.075564             1.692715              0.048814\n",
       "std     1.864304             0.576373              0.009568\n",
       "min     3.574954             0.877036              0.026676\n",
       "50%     7.745580             1.609951              0.048547\n",
       "90%    10.214790             2.152078              0.060033\n",
       "95%    12.067068             2.431052              0.067509\n",
       "99%    12.779571             3.854890              0.073375\n",
       "max    12.936478             4.686015              0.075582"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_max_tokens_df.describe(percentiles = [.5, 0.9, .95, .99])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
