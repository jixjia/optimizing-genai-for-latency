{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Document Processing\n",
    "\n",
    "This case study covers an application which is used to correct a document for spelling errors and grammar mistakes.\n",
    "\n",
    "More advanced applications might be used to implement the specific style guides of an organization, or review documents for statements that create exposure to legal risk, and so on.\n",
    "\n",
    "LLMs are a powerful tool for document processing, however as the documents get longer, the time taken for the application to complete the task becomes increasingly important. If the application is an overnight batch process, this is less of a concern, however if a user is waiting for the output to continue their workflow, then the response time is key to the user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import copy\n",
    "import textwrap\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def aoai_call(system_message,prompt,model):\n",
    "    client = AzureOpenAI(\n",
    "        api_version=os.getenv(\"API_VERSION\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"API_KEY\")\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    e2e_time = end_time - start_time\n",
    "\n",
    "    result=json.loads(completion.model_dump_json(indent=2))\n",
    "    prompt_tokens=result[\"usage\"][\"prompt_tokens\"]\n",
    "    completion_tokens=result[\"usage\"][\"completion_tokens\"]\n",
    "    completion_text=result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    return result,prompt_tokens,completion_tokens,completion_text,e2e_time\n",
    "\n",
    "model=os.getenv(\"MODELGPT432k\")\n",
    "\n",
    "# Read essay from a text file\n",
    "with open('document_with_errors.txt', 'r') as f:\n",
    "    document_with_errors = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Case: LLM used to rewrite document and implement corrections\n",
    "\n",
    "**Time taken: 315 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Prompt Tokens: 3292\n",
      "Total Completion Tokens: 3725\n",
      "Total Cost: $0.6445\n",
      "Total Time Taken: 315.50 seconds\n",
      "Rewritten Document: \n",
      "List of Errors to Correct: []\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Initialize variables\n",
    "total_prompt_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "total_time = 0\n",
    "rewritten_document = \"\"\n",
    "list_of_errors_to_correct = []\n",
    "completion_text_list=[]\n",
    "\n",
    "# Split the text into chunks of words\n",
    "chunks = textwrap.wrap(document_with_errors, 1000)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    system_message=\"\"\"\n",
    "    You are a helpful AI assistant.\n",
    "    \"\"\"\n",
    "    prompt=f\"\"\"\n",
    "    Document to correct:\n",
    "    {chunk}\n",
    "\n",
    "    \"\"\"\n",
    "    prompt=prompt+\"\"\"\n",
    "    The output should be a JSON object. Only return the JSON object, with no comments or additional text.\n",
    "    Use this structure:\n",
    "    {\n",
    "        \"rewritten_document\": \"The document with the errors corrected.\",\n",
    "        \"list_of_errors_to_correct\": [\n",
    "            {\n",
    "                \"explanation_of_error\": \"Think step-by-step about identifying potential spelling errors or grammar issues. Consider all errors together and consider the whole sentence of text before applying a rule.\",\n",
    "                \"incorrect_text\": \"If there is a error fill this with the errors text sub-string, consider the full sentence and context of the incorrect text before filling this in\",\n",
    "                \"fixed_text\": \"Think step by step about the error and then fill this with the fixed version of the text, don't apply any other fixes apart from the error if there is one\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    JSON:\n",
    "    \"\"\"\n",
    "\n",
    "    result,prompt_tokens,completion_tokens,completion_text,e2e_time=aoai_call(system_message,prompt,model)\n",
    "    total_prompt_tokens += prompt_tokens\n",
    "    total_completion_tokens += completion_tokens\n",
    "    total_time += e2e_time\n",
    "    completion_text_list.append(completion_text)\n",
    "\n",
    "import json\n",
    "\n",
    "# Assuming completion_text_list is your list of dictionaries\n",
    "combined_list = {\"list_of_errors_to_correct\":[],\"rewritten_document\":\"\"}\n",
    "\n",
    "for completion_text in completion_text_list:\n",
    "    json_string = completion_text.replace('\\n', '')\n",
    "    data = json.loads(json_string)\n",
    "    if not data[\"list_of_errors_to_correct\"]:\n",
    "        pass\n",
    "    else:\n",
    "        combined_list[\"list_of_errors_to_correct\"].append(data[\"list_of_errors_to_correct\"])\n",
    "        combined_list[\"rewritten_document\"]=combined_list[\"rewritten_document\"]+data[\"rewritten_document\"]\n",
    "\n",
    "document_with_corrections=combined_list[\"rewritten_document\"]\n",
    "# print(document_with_corrections)\n",
    "\n",
    "\n",
    "# Print totals\n",
    "# print(f\"Total Prompt Tokens: {total_prompt_tokens}\")\n",
    "# print(f\"Total Completion Tokens: {total_completion_tokens}\")\n",
    "print(f\"Total Time Taken: {total_time:.2f} seconds\")\n",
    "# print(f\"List of Errors to Correct: {list_of_errors_to_correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the \"avoid-rewriting-documents\" technique\n",
    "\n",
    "**Time taken: 38 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Prompt Tokens: 3255\n",
      "Total Completion Tokens: 421\n",
      "Total Time Taken: 38.15 seconds\n",
      "Rewritten Document: \n",
      "List of Errors to Correct: []\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Initialize variables\n",
    "total_prompt_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "total_time = 0\n",
    "rewritten_document = \"\"\n",
    "list_of_errors_to_correct = []\n",
    "completion_text_list=[]\n",
    "\n",
    "# Split the text into chunks of words\n",
    "chunks = textwrap.wrap(document_with_errors, 1000)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    system_message=\"\"\"\n",
    "    You are a helpful AI assistant.\n",
    "    \"\"\"\n",
    "    prompt=f\"\"\"\n",
    "    Document to correct:\n",
    "    {chunk}\n",
    "\n",
    "    \"\"\"\n",
    "    prompt=prompt+\"\"\"\n",
    "    The output should be a JSON object. Only return the JSON object, with no comments or additional text.\n",
    "    Use this structure:\n",
    "    {\n",
    "        \"list_of_errors_to_correct\": [\n",
    "            {\n",
    "                \"explanation_of_error\": \"Think step-by-step about identifying potential spelling errors or grammar issues. Consider all errors together and consider the whole sentence of text before applying a rule.\",\n",
    "                \"incorrect_text\": \"If there is a error fill this with the errors text sub-string, consider the full sentence and context of the incorrect text before filling this in\",\n",
    "                \"fixed_text\": \"Think step by step about the error and then fill this with the fixed version of the text, don't apply any other fixes apart from the error if there is one\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    JSON:\n",
    "    \"\"\"\n",
    "\n",
    "    result,prompt_tokens,completion_tokens,completion_text,e2e_time=aoai_call(system_message,prompt,model)\n",
    "    total_prompt_tokens += prompt_tokens\n",
    "    total_completion_tokens += completion_tokens\n",
    "    total_time += e2e_time\n",
    "    completion_text_list.append(completion_text)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Assuming completion_text_list is your list of dictionaries\n",
    "combined_list = {\"list_of_errors_to_correct\":[]}\n",
    "\n",
    "for completion_text in completion_text_list:\n",
    "    json_string = completion_text.replace('\\n', '')\n",
    "    data = json.loads(json_string)\n",
    "    if not data[\"list_of_errors_to_correct\"]:\n",
    "        pass\n",
    "    else:\n",
    "        combined_list[\"list_of_errors_to_correct\"].append(data[\"list_of_errors_to_correct\"])\n",
    "\n",
    "# Assuming document_with_errors is your text document\n",
    "for error_list in combined_list[\"list_of_errors_to_correct\"]:\n",
    "    for error_dict in error_list:\n",
    "        document_with_errors = document_with_errors.replace(error_dict['violating_text'], error_dict['fixed_text'])\n",
    "document_with_corrections=document_with_errors\n",
    "# print(document_with_corrections)\n",
    "\n",
    "## Print totals\n",
    "# print(f\"Total Prompt Tokens: {total_prompt_tokens}\")\n",
    "# print(f\"Total Completion Tokens: {total_completion_tokens}\")\n",
    "print(f\"Total Time Taken: {total_time:.2f} seconds\")\n",
    "# print(f\"List of Errors to Correct: {list_of_errors_to_correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the \"prompt-for-concision\" technique\n",
    "\n",
    "**Time taken: 20 seconds**\n",
    "\n",
    "Prompting the model to be concise when explaining the violiations may lead to even more dramatic improvements. In real world examples, this may add up to a 2x improvement.\n",
    "\n",
    "In this example, for simple spelling errors, there is no material difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Prompt Tokens: 6006\n",
      "Total Completion Tokens: 96\n",
      "Total Time Taken: 24.35 seconds\n",
      "Rewritten Document: \n",
      "List of Errors to Correct: []\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Initialize variables\n",
    "total_prompt_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "total_time = 0\n",
    "rewritten_document = \"\"\n",
    "list_of_errors_to_correct = []\n",
    "completion_text_list=[]\n",
    "\n",
    "# Split the text into chunks of words\n",
    "chunks = textwrap.wrap(document_with_errors, 1000)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    system_message=\"\"\"\n",
    "    You are a helpful AI assistant.\n",
    "    \"\"\"\n",
    "    prompt=\"\"\"\n",
    "    The output should be a list object. Only return the list object, with no comments or additional text.\n",
    "    Use this structure:\n",
    "    [[\"The first item in the list is an explanation of the error. Think step-by-step about identifying potential spelling errors or grammar issues. Consider all errors together and consider the whole sentence of text before applying a rule. Explain the error in the shortest sentence possible, ideally 3 to 7 words.\",\"The second item in this list is the specific text that includes the error, typically around 3 words either side of the error. If there is a error fill this with the errors text sub-string, consider the full sentence and context of the incorrect text before filling this in. When selecting the substring of the text, use the shortest amount of text whilst ensuring the sub string is unique in the document.\",\"The third item is the corrected text. It should be exactly the same string as the second item, but with the spelling error or grammar corrected. Think step by step about the error and then fill this with the fixed version of the text.\"]]\n",
    "\n",
    "    START_EXAMPLE_1\n",
    "    \n",
    "    INPUT_DOCUMENT:\n",
    "    Start Your Day Positively: Begin your mornings with small victories. Accomplish a tinny task, like making your bed or enjoying a cup of coffee. Set an intention for the day, whether it’s a guiding principle or a specific action you’ll take. Delayed using your phone upon waking up and replace social media scrolling with gratitude. Reflect on simple things you’re thankful for, like someone holding the door open or a warm cup of coffee from your partner.\n",
    "    Prioritize Self-Care Throughout the Day: As the day progresses, continue nurturing your well-being. Savor encouraging words, read inspirational quotes, or revisit kind texts from friends. Stay active by taking short walks or practicing mindfulness. Remember that small, consistent efforts add up, contributing to your overall health and happiness. \n",
    "    OUTPUT_LIST:[[\"Verb tense\",\"Delayed using your\",\"Delay using your\"],[\"Spelling\",\"a tinny task\",\"a tiny task\"]]\n",
    "\n",
    "    Do not output \\n and many whitespaces.\n",
    "\n",
    "    If there are no errors, return a list like this:[[\"NA\",\"NA\",\"NA\"]]. Do not generate output like this:['\\n    [[\"NA\",\"NA\",\"NA\"]]']\n",
    "    \"\"\"\n",
    "\n",
    "    prompt=prompt+f\"\"\"\n",
    "    INPUT_DOCUMENT:\n",
    "    {chunk}\n",
    "    OUTPUT_LIST:\n",
    "    \"\"\"\n",
    "\n",
    "    result,prompt_tokens,completion_tokens,completion_text,e2e_time=aoai_call(system_message,prompt,model)\n",
    "    total_prompt_tokens += prompt_tokens\n",
    "    total_completion_tokens += completion_tokens\n",
    "    total_time += e2e_time\n",
    "    completion_text_list.append(completion_text)\n",
    "\n",
    "import json\n",
    "\n",
    "# Assuming completion_text_list is your new list of strings\n",
    "combined_list = []\n",
    "\n",
    "for completion_text in completion_text_list:\n",
    "    json_string = completion_text.replace('\\n', '')\n",
    "    data = json.loads(json_string)\n",
    "    if data[0][0] == \"NA\":\n",
    "        pass\n",
    "    else:\n",
    "        combined_list.append(data)\n",
    "\n",
    "# Assuming document_with_errors is your text document\n",
    "for error_list in combined_list:\n",
    "    for error_dict in error_list:\n",
    "        document_with_errors = document_with_errors.replace(error_dict[1], error_dict[2])\n",
    "document_with_corrections = document_with_errors\n",
    "# print(document_with_corrections)\n",
    "\n",
    "\n",
    "# Print totals\n",
    "# print(f\"Total Prompt Tokens: {total_prompt_tokens}\")\n",
    "# print(f\"Total Completion Tokens: {total_completion_tokens}\")\n",
    "print(f\"Total Time Taken: {total_time:.2f} seconds\")\n",
    "# print(f\"Rewritten Document: {rewritten_document}\")\n",
    "# print(f\"List of Errors to Correct: {list_of_errors_to_correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the parallelization technique and make it even more concise\n",
    "\n",
    "**Time taken: 3 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "from aiohttp import ClientSession\n",
    "\n",
    "async def fetch(session, system_message, user_message):\n",
    "    url = f'{os.getenv(\"AZURE_ENDPOINT\")}/openai/deployments/gpt-4/chat/completions?api-version={os.getenv(\"API_VERSION\")}'\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": os.getenv(\"API_KEY\")\n",
    "    }  \n",
    "    data = {\n",
    "        \"model\": \"gpt-4\",  # Adjust the model as needed\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    }\n",
    "    async with session.post(url, json=data, headers=headers) as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def main(system_message, user_messages):\n",
    "    api_call_batch_size = 16\n",
    "\n",
    "    async with ClientSession() as session:\n",
    "        tasks = []\n",
    "        responses = []\n",
    "        for i, user_message in enumerate(user_messages):\n",
    "            # time.sleep(1) # Add a small delay to avoid hitting the rate limit\n",
    "            task = asyncio.create_task(fetch(session, system_message, user_message))\n",
    "            tasks.append(task)\n",
    "            if len(tasks) >= api_call_batch_size: \n",
    "                responses.extend(await asyncio.gather(*tasks))\n",
    "                tasks = []\n",
    "        responses.extend(await asyncio.gather(*tasks))  # Process the last batch\n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 2.819835901260376 seconds\n",
      "['[[\"NA\",\"NA\",\"NA\"]]', '[[\"Spelling\",\"Initial projecctions for\",\"Initial projections for\"]]', '[[\"NA\",\"NA\",\"NA\"]]', '[[\"Plural verb agreement\",\"micro-transactions needs some\",\"micro-transactions need some\"]]', '[[\"NA\",\"NA\",\"NA\"]]', '[[\"NA\", \"NA\", \"NA\"]]', '[[\"NA\",\"NA\",\"NA\"]]', '[[\"Incorrect verb form\",\"We aim to created\",\"We aim to create\"]]', '[[\"NA\",\"NA\",\"NA\"]]']\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# Split the text into chunks of words\n",
    "chunks = textwrap.wrap(document_with_errors, 1000)\n",
    "# Prepend \"INPUT_DOCUMENT: \" and append \" OUTPUT_LIST:\"\n",
    "chunks = [\"INPUT_DOCUMENT: \" + chunk + \" OUTPUT_LIST:\" for chunk in chunks]\n",
    "\n",
    "system_message=\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "\n",
    "###Important :\n",
    "Do not add any additional information.\n",
    "Make sure to complete all elements of the array'''\n",
    "\n",
    "The output should be a list object. Only return the list object, with no comments or additional text.\n",
    "Use this structure:\n",
    "[[\"The first item in the list is an explanation of the error. Think step-by-step about identifying potential spelling errors or grammar issues. Consider all errors together and consider the whole sentence of text before applying a rule. Explain the violation in the shortest sentence possible, ideally 3 to 7 words.\",\"The second item in this list is the specific text that includes the error, typically around 3 words either side of the error. If there is a error fill this with the errors text sub-string, consider the full sentence and context of the incorrect text before filling this in. When selecting the substring of the text, use the shortest amount of text whilst ensuring the sub string is unique in the document.\",\"The third item is the corrected text. It should be exactly the same string as the second item, but with the spelling error or grammar corrected. Think step by step about the error and then fill this with the fixed version of the text.\"]]\n",
    "\n",
    "START_EXAMPLE_1\n",
    "\n",
    "INPUT_DOCUMENT:\n",
    "Start Your Day Positively: Begin your mornings with small victories. Accomplish a tinny task, like making your bed or enjoying a cup of coffee. Set an intention for the day, whether it’s a guiding principle or a specific action you’ll take. Delayed using your phone upon waking up and replace social media scrolling with gratitude. Reflect on simple things you’re thankful for, like someone holding the door open or a warm cup of coffee from your partner.\n",
    "Prioritize Self-Care Throughout the Day: As the day progresses, continue nurturing your well-being. Savor encouraging words, read inspirational quotes, or revisit kind texts from friends. Stay active by taking short walks or practicing mindfulness. Remember that small, consistent efforts add up, contributing to your overall health and happiness. \n",
    "OUTPUT_LIST:[[\"Verb tense\",\"Delayed using your\",\"Delay using your\"],[\"Spelling\",\"a tinny task\",\"a tiny task\"]]\n",
    "\n",
    "Do not output \\n and many whitespaces.\n",
    "\n",
    "If there are no errors, return a list like this:[[\"NA\",\"NA\",\"NA\"]]. Do not generate output like this:['\\n    [[\"NA\",\"NA\",\"NA\"]]']\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# result,prompt_tokens,completion_tokens,completion_text,e2e_time=aoai_batched_call(system_message,json.dumps([chunks]), model)\n",
    "\n",
    "start = time.time()\n",
    "responses_async = await main(system_message, chunks)\n",
    "end = time.time()\n",
    "run_time = end - start\n",
    "print(f\"Total time taken: {run_time} seconds\")\n",
    "\n",
    "# responses\n",
    "completion_text_list = []\n",
    "for i in range(len(responses_async)):\n",
    "    try:\n",
    "        completion_text_list.append(responses_async[i][\"choices\"][0][\"message\"][\"content\"])\n",
    "    except:\n",
    "        pass\n",
    "# print(completion_text_list)\n",
    "# print(len(completion_text_list))\n",
    "# # Print totals\n",
    "# print(f\"Total Prompt Tokens: {prompt_tokens}\")\n",
    "# print(f\"Total Completion Tokens: {completion_tokens}\")\n",
    "# print(f\"Total Time Taken: {e2e_time:.2f} seconds\")\n",
    "# print(f\"Rewritten Document: {completion_text}\")\n",
    "\n",
    "import json\n",
    "\n",
    "# Assuming completion_text_list is your new list of strings\n",
    "combined_list = []\n",
    "\n",
    "for completion_text in completion_text_list:\n",
    "    json_string = completion_text.replace('\\n', '')\n",
    "    data = json.loads(json_string)\n",
    "    if data[0][0] == \"NA\":\n",
    "        pass\n",
    "    else:\n",
    "        combined_list.append(data)\n",
    "\n",
    "# Assuming document_with_errors is your text document\n",
    "for error_list in combined_list:\n",
    "    for error_dict in error_list:\n",
    "        document_with_errors = document_with_errors.replace(error_dict[1], error_dict[2])\n",
    "document_with_corrections = document_with_errors\n",
    "print(document_with_corrections)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
